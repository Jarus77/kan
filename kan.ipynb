{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the B-Spline basis functions\n",
    "class BSpline(nn.Module):\n",
    "    def __init__(self, order=3, knots=None):\n",
    "        super(BSpline, self).__init__()\n",
    "        self.order = order\n",
    "        if knots is None:\n",
    "            self.knots = torch.linspace(0, 1, order + 1)\n",
    "        else:\n",
    "            self.knots = knots\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.basis(x, self.knots, self.order)\n",
    "\n",
    "    def basis(self, x, knots, order):\n",
    "        if order == 0:\n",
    "            return ((knots[:-1] <= x) & (x < knots[1:])).float()\n",
    "        else:\n",
    "            denom1 = knots[order:] - knots[:-order]\n",
    "            denom1[denom1 == 0] = 1\n",
    "            term1 = (x - knots[:-order].unsqueeze(1)) / denom1.unsqueeze(1)\n",
    "\n",
    "            denom2 = knots[order + 1:] - knots[1:-order]\n",
    "            denom2[denom2 == 0] = 1\n",
    "            term2 = (knots[order + 1:].unsqueeze(1) - x) / denom2.unsqueeze(1)\n",
    "\n",
    "            basis_left = self.basis(x, knots, order - 1)[:-1]\n",
    "            basis_right = self.basis(x, knots, order - 1)[1:]\n",
    "\n",
    "            return term1 * basis_left + term2 * basis_right\n",
    "\n",
    "# Define a single KAN layer\n",
    "class KANLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, spline_order=3):\n",
    "        super(KANLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.basis_functions = nn.ModuleList([BSpline(order=spline_order) for _ in range(in_features * out_features)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output = torch.zeros(batch_size, self.out_features).to(x.device)\n",
    "        for i in range(self.out_features):\n",
    "            for j in range(self.in_features):\n",
    "                output[:, i] += self.basis_functions[i * self.in_features + j](x[:, j])\n",
    "        return output\n",
    "\n",
    "# Define the KAN network\n",
    "class KAN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, depth=3, spline_order=3):\n",
    "        super(KAN, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(depth):\n",
    "            layers.append(KANLayer(input_dim, hidden_dim, spline_order))\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:11<00:00, 875391.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 117800.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 915396.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4866035.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1000, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the KAN model, loss function, and optimizer\n",
    "input_dim = 28 * 28  # MNIST images are 28x28\n",
    "output_dim = 10  # MNIST has 10 classes\n",
    "model = KAN(input_dim=input_dim, output_dim=output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, 28 * 28)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28 * 28)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from kan import KAN\n",
    "\n",
    "\n",
    "\n",
    "def test_mul():\n",
    "    kan = KAN([2, 2, 1], base_activation=nn.Identity)\n",
    "    optimizer = torch.optim.LBFGS(kan.parameters(), lr=1)\n",
    "    with tqdm(range(100)) as pbar:\n",
    "        for i in pbar:\n",
    "            loss, reg_loss = None, None\n",
    "\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                x = torch.rand(1024, 2)\n",
    "                y = kan(x, update_grid=(i % 20 == 0))\n",
    "\n",
    "                assert y.shape == (1024, 1)\n",
    "                nonlocal loss, reg_loss\n",
    "                u = x[:, 0]\n",
    "                v = x[:, 1]\n",
    "                loss = nn.functional.mse_loss(y.squeeze(-1), (u + v) / (1 + u * v))\n",
    "                reg_loss = kan.regularization_loss(1, 0)\n",
    "                (loss + 1e-5 * reg_loss).backward()\n",
    "                return loss + reg_loss\n",
    "\n",
    "            optimizer.step(closure)\n",
    "            pbar.set_postfix(mse_loss=loss.item(), reg_loss=reg_loss.item())\n",
    "    for layer in kan.layers:\n",
    "        print(layer.spline_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on MNIST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load MNIST\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "valset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = KAN([28 * 28, 64, 10])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Define optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:17<00:00, 53.73it/s, accuracy=0.906, loss=0.447, lr=0.001] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 0.23048456607588158, Val Accuracy: 0.9325238853503185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:16<00:00, 55.22it/s, accuracy=0.938, loss=0.165, lr=0.0008] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Val Loss: 0.16485021670888753, Val Accuracy: 0.9535230891719745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:17<00:00, 54.18it/s, accuracy=1, loss=0.0532, lr=0.00064]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Val Loss: 0.12734886335214943, Val Accuracy: 0.9625796178343949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 50.08it/s, accuracy=0.969, loss=0.0837, lr=0.000512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Val Loss: 0.11534768961826755, Val Accuracy: 0.9658638535031847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:29<00:00, 32.28it/s, accuracy=0.969, loss=0.0481, lr=0.00041]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Val Loss: 0.1104087866694447, Val Accuracy: 0.9678542993630573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:29<00:00, 31.54it/s, accuracy=0.938, loss=0.153, lr=0.000328] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Val Loss: 0.10152506728798957, Val Accuracy: 0.9694466560509554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:17<00:00, 54.30it/s, accuracy=1, loss=0.015, lr=0.000262]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Val Loss: 0.09775269842501967, Val Accuracy: 0.9705414012738853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:16<00:00, 55.36it/s, accuracy=1, loss=0.0102, lr=0.00021]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Val Loss: 0.09474892176800426, Val Accuracy: 0.971437101910828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:16<00:00, 56.07it/s, accuracy=1, loss=0.034, lr=0.000168]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Val Loss: 0.09255986028220387, Val Accuracy: 0.972531847133758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:16<00:00, 56.17it/s, accuracy=1, loss=0.0177, lr=0.000134]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Val Loss: 0.09221856407732199, Val Accuracy: 0.9726313694267515\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(10):\n",
    "    # Train\n",
    "    model.train()\n",
    "    with tqdm(trainloader) as pbar:\n",
    "        for i, (images, labels) in enumerate(pbar):\n",
    "            images = images.view(-1, 28 * 28).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            accuracy = (output.argmax(dim=1) == labels.to(device)).float().mean()\n",
    "            pbar.set_postfix(loss=loss.item(), accuracy=accuracy.item(), lr=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valloader:\n",
    "            images = images.view(-1, 28 * 28).to(device)\n",
    "            output = model(images)\n",
    "            val_loss += criterion(output, labels.to(device)).item()\n",
    "            val_accuracy += (\n",
    "                (output.argmax(dim=1) == labels.to(device)).float().mean().item()\n",
    "            )\n",
    "    val_loss /= len(valloader)\n",
    "    val_accuracy /= len(valloader)\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
